---

Created at: 2019-05-21
Last updated at: 2019-05-24
tags: 
  - 大数据

---

# 大数据


## 大数据

#### streamingPro简介

[StreamingPro](https://github.com/allwefantasy/streamingpro)主要设计为在Apache Spark上运行，但它也支持运行在Apache FLink。因此，它可以被认为是一个跨平台的分布式平台，它是大数据（BigData）平台和人工智能（AI）平台的结合，可以运行大数据处理和机器学习脚本。这个项目的作者是祝威廉
特征：
![[../_resources/unknown_filename-42d6141b.png]]
体系架构：
![[../_resources/unknown_filename-0d91115a.png]]

#### 环境准备

本次安装需要三台虚拟机，选用的操作系统版本是contos6.9 64位。(虽然StreamingPro只依赖spark，但是本次安装还是将hadoop相关的其他组件一并安装，方便后续使用，例如hbase和hive等)
各组件在三台机器上的分布情况如下：
JDK ：Hadoop和Spark 依赖的配置.
Scala：Spark依赖的配置，建议版本不低于spark的版本。
Hadoop: 是一个分布式系统基础架构。
Spark: 分布式存储的大数据进行处理的工具。
zookeeper:分布式应用程序协调服务，HBase集群需要。
HBase: 一个结构化数据的分布式存储系统。
Hive: 基于Hadoop的一个数据仓库工具，目前的默认元数据库是mysql。
Kafka:分布式消息队列

#### 集群的相关配置

下面前5步操作可以先在一台虚拟机上操作，然后克隆出另外两台机器。

1. 关闭防火墙及关闭操作系统安全访问控制
	说明:其实可以不关闭防火墙，进行权限设置，但是为了方便访问，于是便关闭了防火墙。每个机器都做！！！
	永久关闭防火墙:
	chkconfig iptables off
	说明：操作系统默认是打开安全访问控制的，这样会导致部分软件安装配置失败，为了方便，我们将它关闭。
	查看SELinux状态
	getenforce
	修改配置文件/etc/selinux/config 将SELINUX=enforcing改为SELINUX=disabled

* 修改系统网络配置
	配置固定ip地址和dns服务器地址，保证主机能上网。
* 调整系统时区及时间配置
	首先将时区调整到东八区，保险起见再执行一下下面的命令
	cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
	集群上的机器时间要同步，我们可以将所有的机器跟集群中某台机器进行时间同步。也可以简单的跟互联网授时服务器同步。
	ntpdate pool.ntp.org
* 安装java和scala基础环境及修改系统环境变量
	安装jdk1.8
	解压缩jdk安装包
	tar zxvf jdk-8u144-linux-x64.tar.gz
	创建目录/opt/java
	mkdir /opt/java
	移动文件夹jdk1.8.0_144到/opt/jdk下面，并改名为jdk1.8
	mv jdk1.8.0_144/ /opt/jdk/jdk1.8
	安装scala2.12.2
	tar zxvf scala-2.12.2.tgz
	创建目录/opt/scala
	mkdir /opt/scala
	移动文件夹scala-2.12.2到/opt/ scala下面，并改名为scala2.12.2
	mv scala-2.12.2 /opt/scala/scala2.12.2
	在 /etc/profile 这个配置文件要添加很多的环境配置，这里就先将整体的环境配置列举出来，各位在配置环境变量的以自己的为准！！！ 可以先配置好环境变量之后，在传输到其他机器上去。
	source /etc/profile

    #Java Config
    export JAVA_HOME=/opt/java/jdk1.8
    export JRE_HOME=/opt/java/jdk1.8/jre
    export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib
    # Scala Config
    export SCALA_HOME=/opt/scala/scala2.12.2
    # Spark Config
    export  SPARK_HOME=/opt/spark/spark2.2
    # Zookeeper Config
    export ZK_HOME=/opt/zookeeper/zookeeper3.4
    # HBase Config
    export HBASE_HOME=/opt/hbase/hbase1.2
    # Hadoop Config 
    export HADOOP_HOME=/opt/hadoop/hadoop2.8
    export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
    export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
    # Hive Config
    export HIVE_HOME=/opt/hive/hive2.1
    export HIVE_CONF_DIR=${HIVE_HOME}/conf
    #kafka config
    export KAFKA_HOME=/opt/kafka/kafka1.0.0
  
    export PATH=.:${JAVA_HOME}/bin:${SCALA_HOME}/bin:${SPARK_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${ZK_HOME}/bin:${HBASE_HOME}/bin:${HIVE_HOME}/bin:${KAFKA_HOME}/bin:$PATH
  

5. 主机名更改以及主机和IP做相关映射
	说明:更改主机名是为了方便集群管理，不然每个机器的名称都叫localhost也不太好吧！ 集群所有的机器都要做这个操作。
	输入
	vim /etc/sysconfig/network
	将localhost.localdomain修改为你要更改的名称，每台名称都不一样
	例如:
	HOSTNAME=master
	主机名和IP的关系映射
	修改hosts文件，做关系映射
	说明:这个每台机器都做这个配置，具体ip和主机名称以自己的为准。
	输入:
	vim /etc/hosts
	添加
	192.168.124.201 master
	192.168.124.202 slave1
	192.168.124.203 slave2
	重启机器，开始克隆另外两台机器slave1,slave2,值得注意的是克隆完之后修改修改ip地址和主机名。
6. ssh免密登录
	设置ssh免密码登录是为了操作方便
	生成秘钥文件
	在每台机器上都执行一遍
	首先输入:
	ssh-keygen -t rsa -P ''
	生成秘钥之后，然后将每台机器/root/.ssh 都存入内容相同的文件，文件名称叫authorized_keys，文件内容是我们刚才为3台机器生成的公钥。可以在一台机器上生成，然后复制到其它的机器上。
	新建authorized_keys文件
	输入 :
	touch /root/.ssh/authorized_keys
	编辑 authorized_keys 并将其他机器上的秘钥拷贝过来
	cat /root/.ssh/id_rsa.pub
	vim /root/.ssh/authorized_keys
	将其它机器上的 id_rsa.pub 的内容拷贝到 authorized_keys这个文件中。
	最终authorized_keys文件的内容
	将这个最终的authorized_keys文件copy到其他机器的 /root/.ssh 目录下。使用scp或者ftp都可以。
	scp命令示例:
	scp -r /root/.ssh/authorized_keys root@slave1:/root/.ssh
	scp -r /root/.ssh/authorized_keys root@slave2:/root/.ssh
	测试免密码登录
	输入:
	ssh slave1
	ssh slave2
	输入 exit 退出

#### 开始安装环境

##### Hadoop的环境搭建

事先说明，这些配置可以在一台机器上配置，然后复制到其他机器上就行了。复制之后注意使这些配置文件生效。

1. hadoop配置
	1.1. 文件准备
	将下载下来的Hadoop的配置文件进行解压
	在linux上输入:
	tar -xvf hadoop-2.8.2.tar.gz
	然后将解压之后的文件夹移动到opt/hadoop文件夹下，没有该文件夹就新建，然后将文件夹重命名为hadoop2.8。
	mkdir /opt/hadoop
	在linux上输入移动文件夹命令:
	mv hadoop-2.8.2 /opt/hadoop/hadoop2.8
	1.2. 修改配置文件
	修改 core-site.xml、hadoop-env.sh、hdfs-site.xml、mapred-site.xml 等这些配置文件
	在linux输入进入该目录的命令：
	cd /opt/hadoop/hadoop2.8/etc/hadoop
	1.2.1. 修改 core-site.xml
	hadoop的存放路径可以自行更改。开始我以为这些文件夹需要手动创建，后来实践了，如果不手动创建，会自动创建的，所以就去掉了手动创建目录的步骤。
	输入:
	vim core-site.xml

说明: fs.defaultFS 是缺省文件的名称， 最早使用的是 fs.default.name，后来在最新官方文档中查到该方法已经弃用了。于是边改成这个了。ps：感觉也没啥区别。
1.2.2. 修改 hadoop-env.sh
export JAVA_HOME=${JAVA_HOME}
修改为：
export JAVA_HOME=/opt/java/jdk1.8
注:修改为自己JDK的路径
1.2.3. 修改 hdfs-site.xml
下面的hdfs的存放路径，可以根据自己机器更改。

1.2.4. 修改mapred-site.xml
执行mapreduce的运行框架配置。ps:感觉这个配置没啥用，可能我没用mr吧。
如果没有 mapred-site.xml 该文件，就复制mapred-site.xml.template文件并重命名为mapred-site.xml。
修改这个新建的mapred-site.xml文件


1.2.5. 修改yarn-site.xml文件
yarn 资源调度的配置，集群的话这个配置是必须的。
修改/opt/hadoop/hadoop2.8/etc/hadoop/yarn-site.xml文件，

说明：yarn.nodemanager.vmem-check-enabled这个的意思是忽略虚拟内存的检查，如果你是安装在虚拟机上，这个配置很有用，配上去之后后续操作不容易出问题。如果是实体机上，并且内存够多，可以将这个配置去掉。
1.2.6. 修改slaves
设置主从的配置。如果不设置这个，集群就无法得知主从了。如果是单机模式，就没必要配置了。
修改/opt/hadoop/hadoop2.8/etc/hadoop/slaves文件
更改为
slave1
slave2
在一台机器上(最好是master)做完这些配置之后，我们使用scp命令将这些配置传输到其他机器上。
输入:
hadoop环境传输
scp -r /opt/hadoop root@slave1:/opt
scp -r /opt/hadoop root@slave2:/opt
传输之后，便在主节点启动集群。
在启动hadoop之前，需要初始化，这个只需要在master上初始化就可以了。
2. hadoop启动
注:启动hadoop之前确保防火墙关闭，各个机器时间通过，ssh免登录都没问题。
初始化hadoop （第一次启动hadoop集群时格式化）
切换到/opt/hadoop/hadoop2.8/bin目录下输入
./hdfs namenode -format
初始化成功之后，切换到/opt/hadoop/hadoop2.8/sbin
启动hadoop 的hdfs和yarn
输入:
start-dfs.sh
start-yarn.sh
第一次登录会询问是否连接，输入yes ，然后输入密码就可以了
启动成功之后，可以使用jps命令在各个机器上查看是否成功
可以在浏览器输入: ip+50070 和8088端口查看
http://master:50070
http://master:8088/cluster
若如图显示，则启动成功。
若失败，检查jps是否成功启动，防火墙是否都关闭。都确认没问题之后，还是无法打开界面，请查看日志，再来找原因。
测试：
上传文件到hdfs：hadoop fs -put linux系统文件 hadoop路径
下载文件到本地：hadoop fs -get /tmp/qy88.txt .
删除文件：hadoop fs -rm /tmp/qy87bigfile.txt
创建文件夹：hadoop fs -mkdir /tmp/qy87
删除文件夹：hadoop fs -rmr /tmp/qy87
检查文件是否存在,存在返回0，不存在返回1
常见问题：
19/02/14 14:50:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
如果报上面的警告，原因是系统默认的glibc库是2.12版本，而hadoop期望是2.14版本。
解决办法，在hadoop的log4J配置文件/opt/hadoop/hadoop2.8/etc/hadoop/log4j.properties中加入:
log4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR
master和slave1以及slave2三台机器都要修改。
重启hadoop服务即可。

##### Spark的环境配置

1. 文件准备
	将下载好的Spark文件解压
	输入
	tar zxvf spark-2.2.0-bin-hadoop2.7.tgz
	然后移动到/opt/spark 里面，并重命名
	输入
	mkdir /opt/spark
	mv spark-2.2.0-bin-hadoop2.7 /opt/spark/spark2.2
2. 更改配置文件
	切换目录
	输入: cd /opt/spark/spark2.2/conf/
	2.1 修改 spark-env.sh
	在conf目录下，修改spark-env.sh文件，如果没有 spark-env.sh 该文件，就复制spark-env.sh.template文件并重命名为spark-env.sh。
	修改这个新建的spark-env.sh文件，加入配置:

    export SCALA_HOME=/opt/scala/scala2.12.2   
    export JAVA_HOME=/opt/java/jdk1.8
    export HADOOP_HOME=/opt/hadoop/hadoop2.8    
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop  
    export  SPARK_HOME=/opt/spark/spark2.2
    export SPARK_MASTER_IP=master    
    export SPARK_EXECUTOR_MEMORY=4G
   

注:上面的路径以自己的为准，SPARK_MASTER_IP为主机，SPARK_EXECUTOR_MEMORY为设置的运行内存。
2.2 修改slaves
slaves 分布式文件
在conf目录下，修改slaves文件，如果没有 slaves 该文件，就复制slaves .template文件并重命名为slaves 。
修改这个新建的slaves 文件，加入配置:
slave1
slave2
在一台机器上(最好是master)做完这些配置之后，我们使用scp命令将这些配置传输到其他机器上。
spark环境传输
scp -r /opt/spark root@slave1:/opt
scp -r /opt/spark root@slave2:/opt
传输之后，便在主节点启动集群。
3. spark启动
说明:要先启动Hadoop
切换到Spark目录下
输入:
cd /opt/spark/spark2.2/sbin
然后启动Spark
输入:
start-all.sh
启动成功之后，可以使用jps命令在各个机器上查看是否成功。
可以在浏览器输入: ip+8080 端口查看
若成功显示这个界面，则表示Spark成功启动。

##### Zookeeper的环境配置

因为HBase以及kafka做集群，所以就需要zookeeper了。
zookeeper 在很多环境搭建上，都会有他的身影，如kafka、storm等，这里就不多说了。

1. 文件准备
	将下载下来的Zookeeper 的配置文件进行解压
	在linux上输入:
	tar -xvf zookeeper-3.4.10.tar.gz
	然后移动到/opt/zookeeper里面，没有就新建，然后将文件夹重命名为zookeeper3.4
	输入
	mkdir /opt/zookeeper
	mv zookeeper-3.4.10 /opt/zookeeper/zookeeper3.4
2. 修改配置文件
	2.1 创建文件和目录
	在集群的服务器上都创建这些目录
	mkdir /opt/zookeeper/data
	mkdir /opt/zookeeper/dataLog
	并且在/opt/zookeeper/data目录下创建myid文件
	输入:
	touch myid
	创建成功之后，更改myid文件。
	我这边为了方便，将master、slave1、slave2的myid文件内容改为1,2,3
	2.2 新建zoo.cfg
	切换到/opt/zookeeper/zookeeper3.4/conf 目录下
	如果没有 zoo.cfg 该文件，就复制zoo_sample.cfg文件并重命名为zoo.cfg。
	修改这个新建的zoo.cfg文件

    dataDir=/opt/zookeeper/data
    dataLogDir=/opt/zookeeper/dataLog
    server.1=master:2888:3888
    server.2=slave1:2888:3888
    server.3=slave2:2888:3888
   

配置说明：
client port，顾名思义，就是客户端连接zookeeper服务的端口。这是一个TCP port。
dataLogDir里是放到的顺序日志(WAL)。而dataDir里放的是内存数据结构的snapshot，便于快速恢复。为了达到性能最大化，一般建议把dataDir和dataLogDir分到不同的磁盘上，这样就可以充分利用磁盘顺序写的特性。dataDir和dataLogDir需要自己创建，目录可以自己制定，对应即可。
server.1中的这个1需要和master这个机器上的dataDir目录中的myid文件中的数值对应。server.2中的这个2需要和slave1这个机器上的dataDir目录中的myid文件中的数值对应。server.3中的这个3需要和slave2这个机器上的dataDir目录中的myid文件中的数值对应。当然，数值你可以随便用，只要对应即可。2888和3888的端口号也可以随便用，因为在不同机器上，用成一样也无所谓。
其他配置说明：
1.tickTime：CS通信心跳数
Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。tickTime以毫秒为单位。
tickTime=2000
2.initLimit：LF初始通信时限
集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。
initLimit=10
3.syncLimit：LF同步通信时限
集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数（tickTime的数量）。
syncLimit=5
依旧将zookeeper传输到其他的机器上，记得更改 /opt/zookeeper/data 下的myid，这个不能一致。
输入:
scp -r /opt/zookeeper root@slave1:/opt
scp -r /opt/zookeeper root@slave2:/opt
修改myid
3. 启动zookeeper
因为zookeeper是选举制，它的主从关系并不是像hadoop那样指定的，具体可以看官方的文档说明。
成功配置zookeeper之后，在每台机器上启动zookeeper。
切换到zookeeper目录下
cd /opt/zookeeper/zookeeper3.4/bin
输入:
zkServer.sh start
成功启动之后
查看状态输入:
zkServer.sh status
可以查看各个机器上zookeeper的leader和follower ，只能一个是主，随机选择，入下图：

##### HBase的环境配置

1. 文件准备
	将下载下来的HBase的配置文件进行解压
	在linux上输入:
	tar -xvf hbase-1.2.6-bin.tar.gz
	然后移动到/opt/hbase 文件夹里面，并重命名为 hbase1.2
	输入
	mkdir /opt/hbase
	mv hbase-1.2.6 /opt/hbase/hbase1.2
2. 修改配置文件
	切换到 /opt/hbase/hbase1.2/conf下
	2.1 修改hbase-env.sh
	编辑 hbase-env.sh 文件，添加以下配置

    export JAVA_HOME=/opt/java/jdk1.8
    export HADOOP_HOME=/opt/hadoop/hadoop2.8
    export HBASE_HOME=/opt/hbase/hbase1.2
    export HBASE_CLASSPATH=/opt/hadoop/hadoop2.8/etc/hadoop
    export HBASE_PID_DIR=/root/hbase/pids
    export HBASE_MANAGES_ZK=false
   

说明:配置的路径以自己的为准。HBASE_MANAGES_ZK=false 是不启用HBase自带的Zookeeper集群。
2.2 修改 hbase-site.xml

说明:hbase.rootdir：这个目录是region server的共享目录，用来持久化Hbase 。hbase.cluster.distributed ：Hbase的运行模式。false是单机模式，true是分布式模式。若为false,Hbase和Zookeeper会运行在同一个JVM里面。
2.3 修改regionservers
指定hbase的主从，和hadoop的slaves文件配置一样
将文件修改为
slave1
slave2
注:上面的为集群的主机名称
在一台机器上(最好是master)做完这些配置之后，我们使用scp命令将这些配置传输到其他机器上。
输入:
hbase环境传输
scp -r /opt/hbase root@slave1:/opt
scp -r /opt/hbase root@slave2:/opt
传输之后，在主节点启动集群。
3. 启动hbase
在成功启动Hadoop、zookeeper之后
切换到HBase目录下
cd /opt/hbase/hbase1.2/bin
输入:
start-hbase.sh
启动成功之后，可以使用jps命令在各个机器上查看是否成功
可以在浏览器输入: ip+16010 端口查看
http://master:16010/master-status
若成功显示该界面，则启动成功。

##### Mysql安装

因为Hive的默认元数据是Mysql，所以先要安装Mysql。
Mysql有两种安装模式，可自行选择。

1. yum安装mysql
	首先查看mysql 是否已经安装
	输入：
	rpm -qa | grep mysql
	如果已经安装，想删除的话
	输入:
	普通删除命令:
	rpm -e mysql
	强力删除命令:
	rpm -e --nodeps mysql
	依赖文件也会删除
	安装mysql
	输入:
	yum -y install mysql mysql-server mysql-devel
2. 启动和配置mysql
	安装成功后，输入 service mysqld start 启动服务
	输入之后直接回车(默认是没有密码的)
	然后再输入
	mysql -u root -p
	通过授权法更改远程连接权限
	输入: mysql> GRANT ALL PRIVILEGES ON _._ TO 'root'@'%'IDENTIFIED BY '123456' WITH GRANT OPTION;
	注:第一个’root’是用户名,第二个’%’是所有的ip都可以远程访问,第三个’123456’表示 用户密码 如果不常用 就关闭掉
	use mysql;
	update user set password=passworD("123456") where user='root';
	输入:flush privileges;
	可以使用navcat之类的工具测试是否能正确连接
	将mysql设置为开机启动
	chkconfig mysqld on

##### Hive环境安装和配置

1. 文件准备
	将下载下来的Hive 的配置文件进行解压
	在linux上输入:
	tar -xvf apache-hive-2.1.1-bin.tar.gz
	然后移动到/opt/hive 里面，将文件夹重命名为 hive2.1
	输入
	mkdir /opt/hive
	mv apache-hive-2.1.1-bin /opt/hive/hive2.1
2. 配置更改
	2.1 新建文件夹
	在修改配置文件之前，需要先在root目录下建立一些文件夹。
	mkdir /root/hive
	mkdir /root/hive/warehouse
	新建完该文件之后，需要让hadoop新建/root/hive/warehouse 和 /root/hive/ 目录。
	执行命令：
	_H__A__D__O__O__P__H_​_O__M__E_/_b__i__n_/_h__a__d__o__o__p__f__s_−_m__k__d__i__r_−_p_/_r__o__o__t_/_h__i__v__e_/HADOOP_HOME/bin/hadoop fs -mkdir -p /root/hive/warehouse
	给刚才新建的目录赋予读写权限，执行命令：
	_H__A__D__O__O__P__H_​_O__M__E_/_b__i__n_/_h__a__d__o__o__p__f__s_−_c__h__m__o__d_777/_r__o__o__t_/_h__i__v__e_/HADOOP_HOME/bin/hadoop fs -chmod 777 /root/hive/warehouse
	检查这两个目录是否成功创建
	输入:
	_H__A__D__O__O__P__H_​_O__M__E_/_b__i__n_/_h__a__d__o__o__p__f__s_−_l__s_/_r__o__o__t_/HADOOP_HOME/bin/hadoop fs -ls /root/hive/
	可以看到已经成功创建
	2.2 修改hive-site.xml
	切换到 /opt/hive/hive2.1/conf 目录下
	将hive-default.xml.template 拷贝一份，并重命名为hive-site.xml
	然后编辑hive-site.xml文件
	cp hive-default.xml.template hive-site.xml
	vim hive-site.xml
	编辑hive-site.xml文件:

    <!-- 指定HDFS中的hive仓库地址 -->  
      <property>  
        <name>hive.metastore.warehouse.dir</name>  
        <value>/root/hive/warehouse</value>  
      </property>  
    <property>
        <name>hive.exec.scratchdir</name>
        <value>/root/hive</value>
      </property>
      <!-- 该属性为空表示嵌入模式或本地模式，否则为远程模式 -->  
      <property>  
        <name>hive.metastore.uris</name>  
        <value></value>  
      </property>  
    <!-- 指定mysql的连接 -->
     <property>
            <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true</value>
        </property>
    <!-- 指定驱动类 -->
        <property>
            <name>javax.jdo.option.ConnectionDriverName</name>
            <value>com.mysql.jdbc.Driver</value>
        </property>
       <!-- 指定用户名 -->
        <property>
            <name>javax.jdo.option.ConnectionUserName</name>
            <value>root</value>
        </property>
        <!-- 指定密码 -->
        <property>
            <name>javax.jdo.option.ConnectionPassword</name>
            <value>123456</value>
        </property>
        <property>
       <name>hive.metastore.schema.verification</name>
       <value>false</value>
        <description>
        </description>
     </property>
   

然后将配置文件中所有的
{system:java.io.tmpdir} 更改为 \/opt\/hive\/tmp (如果没有该文件则创建)， 并将此文件夹赋予读写权限， 将{system:user.name}更改为 root
注: 由于hive-site.xml 文件中的配置过多，可以通过FTP将它下载下来进行编辑。也可以直接配置自己所需的，其他的可以删除。 MySQL的连接地址中的master是主机的别名，可以换成ip。
2.3 修改 hive-env.sh
修改hive-env.sh 文件，没有就复制 hive-env.sh.template ，并重命名为hive-env.sh
在这个配置文件中添加

    export  HADOOP_HOME=/opt/hadoop/hadoop2.8
    export  HIVE_CONF_DIR=/opt/hive/hive2.1/conf
    export  HIVE_AUX_JARS_PATH=/opt/hive/hive2.1/lib


2.4 添加 数据驱动包
由于Hive 默认自带的数据库是使用mysql，所以这块就是用mysql
将mysql 的驱动包 上传到 /opt/hive/hive2.1/lib
cp /home/hsy/mysql-connector-java-5.1.41.jar /opt/hive/hive2.1/lib/
3. Hive Shell 测试
在成功启动Hadoop之后
切换到Hive目录下
输入:
cd /opt/hive/hive2.1/bin
3.1 首先初始化数据库
初始化的时候注意要将mysql启动
输入:
schematool -initSchema -dbType mysql
执行成功之后，可以看到hive数据库和一堆表已经创建成功了
3.2 启动hive
进入hive (确保hadoop以及成功启动)输入:
hive
进入hive 之后
做一些简单的操作
新建一个库，然后在建一张表
基本操作和普通的关系型数据库差不多
创建库:
create database db_chenjian;
创建表:
create table db_chenjian.student(id int,name string) row format delimited fields terminated by ' ';
3.3 加载数据
新打开一个窗口
因为hive 不支持写，所以添加数据使用load加载文本获取。
新建一个文本
touch /opt/hive/student.txt
编辑该文本添加数据
输入:
vim /opt/hive/student.txt
添加数据:
中间的空格符使用Tab建
1001 zhangsan
1002 lisi
1003 wangwu
说明: 文本可以在Windows上面新建，然后通过ftp上传到linux中，需要注意文本的格式为unix 格式。
切换到hive shell
加载数据
输入:
load data local inpath '/opt/hive/student.txt' into table db_chenjian.student;
3.4 查询该数据
输入：
select * from db_chenjian.student;

##### 安装配置kafka

1. 文件准备
	将下载下来的kafka 的安装文件进行解压
	在linux上输入:
	tar -zxvf kafka_2.12-1.0.0.tgz
	然后移动到/opt/kafka里面，没有就新建，
	输入
	mkdir /opt/kafka
	mv kafka_2.12-1.0.0 /opt/kafka/kafka1.0.0
2. 修改配置文件
	修改master机器配置文件 /opt/kafka/kafka1.0.0/config/server.properties

        broker.id=0  标示符(多台服务器标示符0,1,2,3，...依次增长)
        host.name=master 绑定的主机
        log.dirs= /opt/kafka/kafka1.0.0/kafka-logs  数据保存的位置
        log.retention.hours=168  数据的保留时间
        zookeeper.connect=master:2181,slave1:2181,slave2:2181
   

将本机kafka拷贝至其他机器，此处只选用了两台机器master和slave1，执行下面的命令：
scp -r /opt/kafka/ root@slave1:/opt/
修改slave1配置文件 /opt/kafka/kafka1.0.0/config/server.properties

        broker.id=1  标示符(多台服务器标示符0,1,2,3，...依次增长)
        host.name=slave1 绑定的主机


启动kafka
a.启动zookeeper集群
b.启动服务的命令(两台机器都执行)
nohup /opt/kafka/kafka1.0.0/bin/kafka-server-start.sh /opt/kafka/kafka1.0.0/config/server.properties &
c.创建主题
/opt/kafka/kafka1.0.0/bin/kafka-topics.sh --create --zookeeper master:2181,slave1:2181 --replication-factor 1 --partitions 1 --topic chenjian
d.查看当前有哪些主题：
/opt/kafka/kafka1.0.0/bin/kafka-topics.sh --list --zookeeper master:2181,slave1:2181,slave2:2181
3. 测试kafka
3.1 模拟发送数据
/opt/kafka/kafka1.0.0/bin/kafka-console-producer.sh --broker-list slave1:9092,slave2:9092 --topic chenjian
3.2 消费数据
/opt/kafka/kafka1.0.0/bin/kafka-console-consumer.sh --zookeeper master:2181,slave1:2181,slave2:2181 --from-beginning --topic chenjian
在模拟发送数据窗口输入，就会看到在消费端有相应的数据。



