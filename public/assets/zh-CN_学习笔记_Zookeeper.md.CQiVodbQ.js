import{_ as t,c as o,a3 as r,o as a}from"./chunks/framework.BaR4VHXY.js";const c=JSON.parse('{"title":"Zookeeper","description":"","frontmatter":{"Created at":"2019-05-24T00:00:00.000Z","Last updated at":"2019-05-24T00:00:00.000Z","tags":["大数据","Zookeeper","应用场景"]},"headers":[],"relativePath":"zh-CN/学习笔记/Zookeeper.md","filePath":"zh-CN/学习笔记/Zookeeper.md","lastUpdated":null}'),n={name:"zh-CN/学习笔记/Zookeeper.md"};function i(l,e,d,s,p,h){return a(),o("div",null,e[0]||(e[0]=[r(`<h1 id="zookeeper" tabindex="-1">Zookeeper <a class="header-anchor" href="#zookeeper" aria-label="Permalink to &quot;Zookeeper&quot;">​</a></h1><h2 id="zookeeper-1" tabindex="-1">Zookeeper <a class="header-anchor" href="#zookeeper-1" aria-label="Permalink to &quot;Zookeeper&quot;">​</a></h2><p>ZooKeeper是一个高可用的分布式数据管理与系统协调框架。基于对Paxos算法的实现，使该框架保证了分布式环境中数据的强一致性，也正是基于这样的特性，使得ZooKeeper解决很多分布式问题。网上对ZK的应用场景也有不少介绍，本文将结合作者身边的项目例子，系统地对ZK的应用场景进行一个分门归类的介绍。 值得注意的是，ZK并非天生就是为这些应用场景设计的，都是后来众多开发者根据其框架的特性，利用其提供的一系列API接口（或者称为原语集），摸索出来的典型使用方法。因此，也非常欢迎读者分享你在ZK使用上的奇技淫巧。 ![[../_resources/unknown_filename-cd240df1.png]]</p><h5 id="zookeeper-的由来" tabindex="-1">ZooKeeper 的由来 <a class="header-anchor" href="#zookeeper-的由来" aria-label="Permalink to &quot;ZooKeeper 的由来&quot;">​</a></h5><p>Zookeeper最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。 关于“ZooKeeper”这个项目的名字，其实也有一段趣闻。在立项初期，考虑到之前内部很多项目都是使用动物的名字来命名的（例如著名的Pig项目),雅虎的工程师希望给这个项目也取一个动物的名字。时任研究院的首席科学家RaghuRamakrishnan（罗摩克里希纳）开玩笑地说：“在这样下去，我们这儿就变成动物园了！”此话一出，大家纷纷表示就叫动物园管理员吧一一一因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而Zookeeper正好要用来进行分布式环境的协调一一于是，Zookeeper的名字也就由此诞生了。</p><h5 id="zookeeper-概览" tabindex="-1">ZooKeeper 概览 <a class="header-anchor" href="#zookeeper-概览" aria-label="Permalink to &quot;ZooKeeper 概览&quot;">​</a></h5><p>![[../_resources/unknown_filename-39c7bb9a.png]] Zookeeper 一个最常用的使用场景就是用于担任服务生产者和服务消费者的注册中心。 服务生产者将自己提供的服务注册到Zookeeper中心，服务的消费者在进行服务调用的时候先到Zookeeper中查找服务，获取到服务生产者的详细信息之后，再去调用服务生产者的内容与数据。如下图所示，在 Dubbo架构中 Zookeeper 就担任了注册中心这一角色。 ![[../_resources/unknown_filename-19c2f944.png]]</p><h4 id="zookeeper原理" tabindex="-1">zookeeper原理 <a class="header-anchor" href="#zookeeper原理" aria-label="Permalink to &quot;zookeeper原理&quot;">​</a></h4><p>Zookeeper虽然在配置文件中并没有指定master和slave 但是，zookeeper工作时，是有一个节点为leader，其他则为follower Leader是通过内部的选举机制临时产生的</p><h5 id="zookeeper全新集群的选举机制" tabindex="-1">zookeeper全新集群的选举机制 <a class="header-anchor" href="#zookeeper全新集群的选举机制" aria-label="Permalink to &quot;zookeeper全新集群的选举机制&quot;">​</a></h5><p>![[../_resources/unknown_filename-f275f360.png]] 以一个简单的例子来说明整个选举的过程. 假设有三台服务器组成的zookeeper集群,它们的id从1-3,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么.</p><ul><li><p>服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态</p></li><li><p>服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器胜出,此时投票次数超过了总投票次数的一半，所以第二台机器为leader。</p></li></ul><h5 id="非全新集群的选举机制-数据恢复" tabindex="-1">非全新集群的选举机制(数据恢复) <a class="header-anchor" href="#非全新集群的选举机制-数据恢复" aria-label="Permalink to &quot;非全新集群的选举机制(数据恢复)&quot;">​</a></h5><p>那么，初始化的时候，是按照上述的说明进行选举的，但是当zookeeper运行了一段时间之后，有机器down掉，重新选举时，选举过程就相对复杂了。 需要加入数据id、leader id和逻辑时钟。 数据id：数据新的id就大，数据每次更新都会更新id。 Leader id：就是我们配置的myid中的值，每个机器一个。 逻辑时钟：这个值从0开始递增,每次选举对应一个值,也就是说: 如果在同一次选举中,那么这个值应该是一致的 ; 逻辑时钟值越大,说明这一次选举leader的进程更新. 选举的标准就变成：</p><ul><li>逻辑时钟小的选举结果被忽略，重新投票</li><li>统一逻辑时钟后，数据id大的胜出</li><li>数据id相同的情况下，leader id大的胜出</li></ul><p>根据这个规则选出leader。</p><h4 id="zookeeper结构和命令" tabindex="-1">zookeeper结构和命令 <a class="header-anchor" href="#zookeeper结构和命令" aria-label="Permalink to &quot;zookeeper结构和命令&quot;">​</a></h4><h5 id="zookeeper特性" tabindex="-1">zookeeper特性 <a class="header-anchor" href="#zookeeper特性" aria-label="Permalink to &quot;zookeeper特性&quot;">​</a></h5><ol><li>Zookeeper：一个leader，多个follower组成的集群</li><li>全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的</li><li>分布式读写，更新请求转发，由leader实施</li><li>更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行</li><li>数据更新原子性，一次数据更新要么成功，要么失败</li><li>实时性，在一定时间范围内，client能读到最新数据</li></ol><h5 id="zookeeper数据结构" tabindex="-1">zookeeper数据结构 <a class="header-anchor" href="#zookeeper数据结构" aria-label="Permalink to &quot;zookeeper数据结构&quot;">​</a></h5><ol><li>层次化的目录结构，命名符合常规文件系统规范(见下图)</li><li>每个节点在zookeeper中叫做znode,并且其有一个唯一的路径标识</li><li>节点Znode可以包含数据和子节点</li></ol><h5 id="数据结构的图" tabindex="-1">数据结构的图 <a class="header-anchor" href="#数据结构的图" aria-label="Permalink to &quot;数据结构的图&quot;">​</a></h5><p>![[../_resources/unknown_filename-2c257a18.png]]</p><h5 id="节点类型" tabindex="-1">节点类型 <a class="header-anchor" href="#节点类型" aria-label="Permalink to &quot;节点类型&quot;">​</a></h5><ol><li>Znode有两种类型： 短暂（ephemeral）（断开连接自己删除） 持久（persistent）（断开连接不删除）</li><li>Znode有四种形式的目录节点（默认是persistent ） PERSISTENT PERSISTENT_SEQUENTIAL（持久序列/test0000000019 ） EPHEMERAL EPHEMERAL_SEQUENTIAL</li><li>创建znode时设置顺序标识,znode名称后会附加一个值,顺序号是一个单调递增的计数器,由父节点维护</li><li>在分布式系统中,顺序号可以被用于为所有的事件进行全局排序,这样客户端可以通过顺序号推断事件的顺序</li></ol><p>ZooKeeper的数据模型是内存中的一个ZNode数,由斜杠(/)进行分割的路径,就是一个ZNode,每个ZNode上除了保存自己的数据内容,还保存一系列属性信息; ZooKeeper中的数据节点分为两种:持久节点和临时节点。 所谓的持久节点是指一旦这个ZNode创建成功,除非主动进行ZNode的移除操作,节点会一直保存在ZooKeeper上;而临时节点的生命周期是跟客户端的会话相关联的,一旦客户端会话失效，这个会话上的所有临时节点都会被自动移除。</p><h5 id="zookeeper命令行操作" tabindex="-1">zookeeper命令行操作 <a class="header-anchor" href="#zookeeper命令行操作" aria-label="Permalink to &quot;zookeeper命令行操作&quot;">​</a></h5><p>运行 zkCli.sh –server ip进入命令行工具 在linux cli输入：./zkCli.sh -server hostIP/hostName</p><p>[zk: master(CONNECTED) 0]说明已经进入zookeeper命令行 ![[../_resources/unknown_filename-2544e0f0.png]]</p><ol><li>使用 ls 命令来查看当前 ZooKeeper 中所包含的内容:ls / ![[../_resources/unknown_filename-99482cf0.png]]</li><li>创建一个新的 znode,使用 create /zk myData 。这个命令创建了一个新的 znode 节点“ zk ”以及与它关联的字符串:create /zk &quot;myData&quot; ![[../_resources/unknown_filename-c46ef520.png]]</li><li>我们运行 get 命令来确认 znode 是否包含我们所创建的字符串:get /zk ![[../_resources/unknown_filename-c0dba317.png]]</li></ol><table tabindex="0"><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>czxid</td><td>节点创建时的zxid.</td></tr><tr><td>mzxid</td><td>节点最新一次更新发生时的zxid.</td></tr><tr><td>ctime</td><td>节点创建时的时间戳.</td></tr><tr><td>mtime</td><td>节点最新一次更新发生时的时间戳.</td></tr><tr><td>pZxid</td><td>这个节点就和子节点有关啦！是与 该节点的子节点（或该节点）的最近一次 创建 / 删除 的时间戳对应</td></tr><tr><td>dataVersion</td><td>节点数据的更新次数.</td></tr><tr><td>cversion</td><td>其子节点的更新次数.</td></tr><tr><td>aclVersion</td><td>节点ACL(授权信息)的更新次数.</td></tr><tr><td>ephemeralOwner</td><td>如果该节点为ephemeral节点, ephemeralOwner值表示与该节点绑定的session id. 如果该节点不是ephemeral节点, ephemeralOwner值为0x0. 至于什么是ephemeral节点, 请看后面的讲述.</td></tr><tr><td>dataLength</td><td>节点数据的字节数.</td></tr><tr><td>numChildren</td><td>子节点个数.</td></tr></tbody></table><p>修改节点值之后mzxid和mTime发生变化： ![[../_resources/unknown_filename-0584c3c9.png]] #监听这个节点的变化,当另外一个客户端改变/zk时,它会打出下面的 #WATCHER:: #WatchedEvent state:SyncConnected type:NodeDataChanged path:/zk get /zk watch 4. 下面我们通过 set 命令来对 zk 所关联的字符串进行设置:set /zk &quot;aaa&quot; 5. 下面我们将刚才创建的 znode 删除:delete /zk ![[../_resources/unknown_filename-649b2a9d.png]] 6. 删除节点(包含子节点):rmr /zk ![[../_resources/unknown_filename-2ffc8c1e.png]] 7. 短暂节点ephemeral和持久节点persistent的区别: 创建短暂节点: ![[../_resources/unknown_filename-e43942ce.png]] 然后使用close断开连接: ![[../_resources/unknown_filename-a65efa87.png]] 发现在其他机器上,close之前可以正常获取节点信息,但是close之后无法获取节点信息。如下图: ![[../_resources/unknown_filename-257d1902.png]]</p><h4 id="zookeeper-api应用" tabindex="-1">zookeeper-api应用 <a class="header-anchor" href="#zookeeper-api应用" aria-label="Permalink to &quot;zookeeper-api应用&quot;">​</a></h4><h5 id="基本使用" tabindex="-1">基本使用 <a class="header-anchor" href="#基本使用" aria-label="Permalink to &quot;基本使用&quot;">​</a></h5><p>org.apache.zookeeper.Zookeeper是客户端入口主类,负责建立与server的会话。</p><table tabindex="0"><thead><tr><th>功能</th><th>描述</th></tr></thead><tbody><tr><td>create</td><td>在本地目录树中创建一个节点</td></tr><tr><td>delete</td><td>删除一个节点</td></tr><tr><td>exists</td><td>测试本地是否存在目标节点</td></tr><tr><td>get/set data</td><td>从目标节点上读取 / 写数据</td></tr><tr><td>get/set ACL</td><td>获取 / 设置目标节点访问控制列表信息</td></tr><tr><td>get children</td><td>检索一个子节点上的列表</td></tr><tr><td>sync</td><td>等待要被传送的数据</td></tr></tbody></table><h5 id="增删改查-springboot" tabindex="-1">增删改查（springboot） <a class="header-anchor" href="#增删改查-springboot" aria-label="Permalink to &quot;增删改查（springboot）&quot;">​</a></h5><pre><code>/**
 * 测试zookeeperAPI
 */
public class MyZookeeper {
    //zookeeper实例
    ZooKeeper zk=null;
    //sessionTimeout连接超时时间
    int sessionTimeout=30000;
    Watcher wc= new Watcher() {
        @Override
        public void process(WatchedEvent event) {

        }
    };
    /**
     * 初始化zookeeper实例
     */
    @Before
    public  void initZookeeperConnect() throws IOException {
        //String connectString, int sessionTimeout, Watcher watcher
        zk= new ZooKeeper(&quot;192.168.153.101:2181&quot;,sessionTimeout,wc);
    }

    /**
     * 创建节点
     */
    @Test
    public  void createNode() throws KeeperException, InterruptedException {
        //第一个参数path：the path for the node节点的路径（名称）
        //第二个参数data：the initial data for the node（节点值）
        //第三个参数acl：the acl for the node(节点的访问控制。权限)
        //第四个参数createMode：specifying whether the node to be created is ephemeral and/or sequential
        // 节点的类型，短暂还是持久
        zk.create(&quot;/newNode&quot;,&quot;hello&quot;.getBytes(),ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT);
    }
    /**
     * 查看节点
     */
    @Test
    public  void getNodeInfo() throws KeeperException, InterruptedException {
        //String path 节点路径, boolean watch是否监控, Stat stat节点状态
        byte[] zkData = zk.getData(&quot;/newNode&quot;, false, null);
        String result=new String(zkData);
        System.out.println(&quot;返回结果：&quot;+result);
    }
    /**
     * 修改节点
     */
    @Test
    public  void setNodeInfo() throws KeeperException, InterruptedException {
        // String path 节点路径, byte data[] 节点值, int version 版本号 默认-1
        zk.setData(&quot;/newNode&quot;,&quot;xxx&quot;.getBytes(),-1);
        //String path 节点路径, boolean watch是否监控, Stat stat节点状态
        byte[] zkData = zk.getData(&quot;/newNode&quot;, false, null);
        String result=new String(zkData);
        System.out.println(&quot;返回结果：&quot;+result);
    }
    /**
     * 删除节点
     */
    @Test
    public  void deleteNodeInfo() throws KeeperException, InterruptedException {
        // String path 节点路径,  int version 版本号 默认-1
        zk.delete(&quot;/newNode&quot;,-1);
        //String path 节点路径, boolean watch是否监控, Stat stat节点状态
        byte[] zkData = zk.getData(&quot;/newNode&quot;, false, null);
        String result=new String(zkData);
        System.out.println(&quot;返回结果：&quot;+result);
    }
    /**
     * 关闭zookeeper连接
     * @throws InterruptedException
     */
    @After
    public void closeZookeeper() throws InterruptedException {
        zk.close();
    }
}
</code></pre><h4 id="zookeeper典型应用场景一览" tabindex="-1">ZooKeeper典型应用场景一览 <a class="header-anchor" href="#zookeeper典型应用场景一览" aria-label="Permalink to &quot;ZooKeeper典型应用场景一览&quot;">​</a></h4><h5 id="数据发布与订阅-配置中心" tabindex="-1">数据发布与订阅（配置中心） <a class="header-anchor" href="#数据发布与订阅-配置中心" aria-label="Permalink to &quot;数据发布与订阅（配置中心）&quot;">​</a></h5><p>发布与订阅模型，即所谓的配置中心，顾名思义就是发布者将数据发布到ZK节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。例如全局的配置信息，服务式服务框架的服务地址列表等就非常适合使用。 注意：在下面提到的应用场景中，有个默认前提是：数据量很小，但是数据更新可能会比较快的场景。</p><ul><li>应用中用到的一些配置信息放到ZK上进行集中管理。这类场景通常是这样：应用在启动的时候会主动来获取一次配置，同时，在节点上注册一个Watcher，这样一来，以后每次配置有更新的时候，都会实时通知到订阅的客户端，从来达到获取最新配置信息的目的。</li><li>分布式搜索服务中，索引的元信息和服务器集群机器的节点状态存放在ZK的一些指定节点，供各个客户端订阅使用。</li><li>分布式日志收集系统。这个系统的核心工作是收集分布在不同机器的日志。收集器通常是按照应用来分配收集任务单元，因此需要在ZK上创建一个以应用名作为path的节点P，并将这个应用的所有机器ip，以子节点的形式注册到节点P上，这样一来就能够实现机器变动的时候，能够实时通知到收集器调整任务分配。</li><li>系统中有些信息需要动态获取，并且还会存在人工手动去修改这个信息的发问。通常是暴露出接口，例如JMX接口，来获取一些运行时的信息。引入ZK之后，就不用自己实现一套方案了，只要将这些信息存放到指定的ZK节点上即可。</li></ul><h5 id="负载均衡" tabindex="-1">负载均衡 <a class="header-anchor" href="#负载均衡" aria-label="Permalink to &quot;负载均衡&quot;">​</a></h5><p>这里说的负载均衡是指软负载均衡。在分布式环境中，为了保证高可用性，通常同一个应用或同一个服务的提供方都会部署多份，达到对等服务。而消费者就须要在这些对等的服务器中选择一个来执行相关的业务逻辑，其中比较典型的是消息中间件中的生产者，消费者负载均衡。 消息中间件中发布者和订阅者的负载均衡，linkedin开源的KafkaMQ和阿里开源的metaq都是通过zookeeper来做到生产者、消费者的负载均衡。这里以metaq为例如讲下：</p><ul><li>生产者负载均衡：metaq发送消息的时候，生产者在发送消息的时候必须选择一台broker上的一个分区来发送消息，因此metaq在运行过程中，会把所有broker和对应的分区信息全部注册到ZK指定节点上，默认的策略是一个依次轮询的过程，生产者在通过ZK获取分区列表之后，会按照brokerId和partition的顺序排列组织成一个有序的分区列表，发送的时候按照从头到尾循环往复的方式选择一个分区来发送消息。</li><li>消费负载均衡： 在消费过程中，一个消费者会消费一个或多个分区中的消息，但是一个分区只会由一个消费者来消费。MetaQ的消费策略是： 每个分区针对同一个group只挂载一个消费者。 如果同一个group的消费者数目大于分区数目，则多出来的消费者将不参与消费。 如果同一个group的消费者数目小于分区数目，则有部分消费者需要额外承担消费任务。</li></ul><p>在某个消费者故障或者重启等情况下，其他消费者会感知到这一变化（通过 zookeeper watch消费者列表），然后重新进行负载均衡，保证所有的分区都有消费者进行消费。</p><h5 id="命名服务-naming-service" tabindex="-1">命名服务(Naming Service) <a class="header-anchor" href="#命名服务-naming-service" aria-label="Permalink to &quot;命名服务(Naming Service)&quot;">​</a></h5><p>命名服务也是分布式系统中比较常见的一类场景。在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务地址，远程对象等等——这些我们都可以统称他们为名字（Name）。其中较为常见的就是一些分布式服务框架中的服务地址列表。通过调用ZK提供的创建节点的 API，能够很容易创建一个全局唯一的path，这个path就可以作为一个名称。 阿里巴巴集团开源的分布式服务框架Dubbo中使用ZooKeeper来作为其命名服务，维护全局的服务地址列表，点击这里查看Dubbo开源项目。在Dubbo实现中：</p><ul><li>服务提供者在启动的时候，向ZK上的指定节点/dubbo/\${serviceName}/providers目录下写入自己的URL地址，这个操作就完成了服务的发布。</li><li>服务消费者启动的时候，订阅/dubbo/{serviceName}/providers目录下的提供者URL地址， 并向/dubbo/{serviceName} /consumers目录下写入自己的URL地址。</li></ul><p>注意，所有向ZK上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。 另外，Dubbo还有针对服务粒度的监控，方法是订阅/dubbo/\${serviceName}目录下所有提供者和消费者的信息。</p><h5 id="分布式通知-协调" tabindex="-1">分布式通知/协调 <a class="header-anchor" href="#分布式通知-协调" aria-label="Permalink to &quot;分布式通知/协调&quot;">​</a></h5><p>ZooKeeper中特有watcher注册与异步通知机制，能够很好的实现分布式环境下不同系统之间的通知与协调，实现对数据变更的实时处理。使用方法通常是不同系统都对ZK上同一个znode进行注册，监听znode的变化（包括znode本身内容及子节点的），其中一个系统update了znode，那么另一个系统能够收到通知，并作出相应处理</p><ul><li>另一种心跳检测机制：检测系统和被检测系统之间并不直接关联起来，而是通过zk上某个节点关联，大大减少系统耦合。</li><li>另一种系统调度模式：某系统有控制台和推送系统两部分组成，控制台的职责是控制推送系统进行相应的推送工作。管理人员在控制台作的一些操作，实际上是修改了ZK上某些节点的状态，而ZK就把这些变化通知给他们注册Watcher的客户端，即推送系统，于是，作出相应的推送任务。</li><li>另一种工作汇报模式：一些类似于任务分发系统，子任务启动后，到zk来注册一个临时节点，并且定时将自己的进度进行汇报（将进度写回这个临时节点），这样任务管理者就能够实时知道任务进度。</li></ul><p>总之，使用zookeeper来进行分布式通知和协调能够大大降低系统之间的耦合</p><h5 id="集群管理与master选举" tabindex="-1">集群管理与Master选举 <a class="header-anchor" href="#集群管理与master选举" aria-label="Permalink to &quot;集群管理与Master选举&quot;">​</a></h5><ul><li>集群机器监控：这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群机器是否存活。过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报“我还活着”。 这种做法可行，但是存在两个比较明显的问题： 1.集群中机器有变动的时候，牵连修改的东西比较多。 2.有一定的延时。 利用ZooKeeper有两个特性，就可以实时另一种集群机器存活性监控系统： 1.客户端在节点 x 上注册一个Watcher，那么如果 x?的子节点变化了，会通知该客户端。 2.创建EPHEMERAL类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。 例如，监控系统在 /clusterServers 节点上注册一个Watcher，以后每动态加机器，那么就往 /clusterServers 下创建一个 EPHEMERAL类型的节点：/clusterServers/{hostname}. 这样，监控系统就能够实时知道机器的增减情况，至于后续处理就是监控系统的业务了。</li><li>Master选举则是zookeeper中最为经典的应用场景了。 在分布式环境中，相同的业务应用分布在不同的机器上，有些业务逻辑（例如一些耗时的计算，网络I/O处理），往往只需要让整个集群中的某一台机器进行执行，其余机器可以共享这个结果，这样可以大大减少重复劳动，提高性能，于是这个master选举便是这种场景下的碰到的主要问题。 利用ZooKeeper的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性，即：同时有多个客户端请求创建 /currentMaster 节点，最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很轻易的在分布式环境中进行集群选取了。 另外，这种场景演化一下，就是动态Master选举。这就要用到?EPHEMERAL_SEQUENTIAL类型节点的特性了。 上文中提到，所有客户端创建请求，最终只有一个能够创建成功。在这里稍微变化下，就是允许所有请求都能够创建成功，但是得有个创建顺序，于是所有的请求最终在ZK上创建结果的一种可能情况是这样： /currentMaster/{sessionId}-1 ,?/currentMaster/{sessionId}-2 ,?/currentMaster/{sessionId}-3 ….. 每次选取序列号最小的那个机器作为Master，如果这个机器挂了，由于他创建的节点会马上小时，那么之后最小的那个机器就是Master了。</li><li>在搜索系统中，如果集群中每个机器都生成一份全量索引，不仅耗时，而且不能保证彼此之间索引数据一致。因此让集群中的Master来进行全量索引的生成，然后同步到集群中其它机器。另外，Master选举的容灾措施是，可以随时进行手动指定master，就是说应用在zk在无法获取master信息时，可以通过比如http方式，向一个地方获取master。</li><li>在Hbase中，也是使用ZooKeeper来实现动态HMaster的选举。在Hbase实现中，会在ZK上存储一些ROOT表的地址和 HMaster的地址，HRegionServer也会把自己以临时节点（Ephemeral）的方式注册到Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的存活状态，同时，一旦HMaster出现问题，会重新选举出一个HMaster来运行，从而避免了 HMaster的单点问题。</li></ul><h5 id="分布式锁" tabindex="-1">分布式锁 <a class="header-anchor" href="#分布式锁" aria-label="Permalink to &quot;分布式锁&quot;">​</a></h5><p>分布式锁，这个主要得益于ZooKeeper为我们保证了数据的强一致性。锁服务可以分为两类，一个是保持独占，另一个是控制时序。</p><ul><li>所谓保持独占，就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。通常的做法是把zk上的一个znode看作是一把锁，通过 create znode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。</li><li>控制时序，就是所有视图来获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序了。做法和上面基本类似，只是这里 /distribute_lock 已经预先存在，客户端在它下面创建临时有序节点（这个可以通过节点的属性控制：CreateMode.EPHEMERAL_SEQUENTIAL来指定）。Zk的父节点（/distribute_lock）维持一份sequence,保证子节点创建的时序性，从而也形成了每个客户端的全局时序。</li></ul><h5 id="分布式队列" tabindex="-1">分布式队列 <a class="header-anchor" href="#分布式队列" aria-label="Permalink to &quot;分布式队列&quot;">​</a></h5><p>队列方面，简单地讲有两种，一种是常规的先进先出队列，另一种是要等到队列成员聚齐之后的才统一按序执行。对于第一种先进先出队列，和分布式锁服务中的控制时序场景基本原理一致，这里不再赘述。 第二种队列其实是在FIFO队列的基础上作了一个增强。通常可以在 /queue 这个znode下预先建立一个/queue/num 节点，并且赋值为n（或者直接给/queue赋值n），表示队列大小，之后每次有队列成员加入后，就判断下是否已经到达队列大小，决定是否可以开始执行了。这种用法的典型场景是，分布式环境中，一个大任务Task A，需要在很多子任务完成（或条件就绪）情况下才能进行。这个时候，凡是其中一个子任务完成（就绪），那么就去 /taskList 下建立自己的临时时序节点（CreateMode.EPHEMERAL_SEQUENTIAL），当 /taskList 发现自己下面的子节点满足指定个数，就可以进行下一步按序进行处理了。</p>`,61)]))}const k=t(n,[["render",i]]);export{c as __pageData,k as default};
